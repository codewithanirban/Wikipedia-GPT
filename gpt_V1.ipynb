{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o33_So55w-du"
      },
      "source": [
        "# Building a GPT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azGWvprsxoMj"
      },
      "source": [
        "Importing the dataset from kaggle --- I've used Wikipedia Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_na_RonwOso",
        "outputId": "23b81a09-7143-4be1-9cae-822d0f2c1470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.12).\n",
            "Path to dataset files: C:\\Users\\dccha\\.cache\\kagglehub\\datasets\\mikeortman\\wikipedia-sentences\\versions\\3\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mikeortman/wikipedia-sentences\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGxmZiEFxvaw",
        "outputId": "c9f4cf1e-79a9-4b86-c486-9ef632eda8f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dccha\\.cache\\kagglehub\\datasets\\mikeortman\\wikipedia-sentences\\versions\\3\\wikisent2.txt\n"
          ]
        }
      ],
      "source": [
        "# set the path dir\n",
        "import os\n",
        "base_path = os.path.join(path, 'wikisent2.txt')\n",
        "print(base_path)\n",
        "# read it in to inspect it\n",
        "with open(base_path,'r',encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDVkheFuyHX6",
        "outputId": "e4a297a4-efed-49a1-ea4c-e01cb1a5a519"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of dataset in characters:  934571982\n"
          ]
        }
      ],
      "source": [
        "print(\"Length of dataset in characters: \",len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NlEuMFny1Pc",
        "outputId": "e64f15a6-a15f-4ebb-d05a-20f4e7ff1d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.000123, which corresponds to a distance of 705 Mly, or 216 Mpc.\n",
            "000webhost is a free web hosting service, operated by Hostinger.\n",
            "0010x0010 is a Dutch-born audiovisual artist, currently living in Los Angeles.\n",
            "0-0-1-3 is an alcohol abuse prevention program developed in 2004 at Francis E. Warren Air Force Base based on research by the National Institute on Alcohol Abuse and Alcoholism regarding binge drinking in college students.\n",
            "0.01 is the debut studio album of H3llb3nt, released on February 20, 1996 by Fifth Colvmn Records.\n",
            "001 of 3 February 1997, which was signed between the Government of the Republic of Rwanda, and FAPADER.\n",
            "003230 is a South Korean food manufacturer.\n",
            "0.04%Gas molecules in soil are in continuous thermal motion according to the kinetic theory of gasses, there is also collision between molecules - a random walk.\n",
            "0.04% of the votes were invalid.\n",
            "005.1999.06 is the fifth studio album by the South Korean singer and actress Uhm Jung-hwa.\n",
            "005 is a 1981 arcade game by Sega.\n",
            "007 Legends is a first-person shooter video game featuring the character of British secret agent James Bond.\n",
            "007 Legends is the fourth and final James Bond game title released by Activision, the last game Eurocom developed before the company ceased operations and also the last James Bond video game to be available on home video game systems, to date.\n",
            "007 Racing is a racing video game based on the James Bond license.\n",
            "00 AM PST and burned a total of 8,110 acres.\n",
            "00am to 1:00pm on TV5, and it is the biggest project to date this year by the network and hosted by majority of the network's bunch of talents collectively known as \"HappyPeeps\".\n",
            "0.0 is a live album by Melt-Banana that came out on November 3, 2009.Melt-Banana perform some of their shows under the name Melt-Banana Lite, which means that they perform their music with a different arrangement.\n",
            "0.0% is considered to be the start of the retracement, while 100.0% is a complete reversal to the original part of the move.\n",
            "00 pm in daily g\n"
          ]
        }
      ],
      "source": [
        "# let us see the first 1500 characters\n",
        "print(text[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVX5E0XCy8GG",
        "outputId": "7898e0fe-714f-4a7e-97d4-d484d21f00a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n",
            "Vocabulary size :  96\n"
          ]
        }
      ],
      "source": [
        "# listing the unique characters that occur in text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(\"Vocabulary size : \",vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars[42]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "443R9l20zQPm",
        "outputId": "85aef6b9-d01f-4ac0-ccc5-9e6c27ed4f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[42, 1, 66, 78, 1, 34, 79, 74, 83, 67, 66, 79, 1, 36, 73, 66, 76, 83, 66, 67, 80, 83, 85, 90]\n",
            "I am Anirban Chakraborty\n"
          ]
        }
      ],
      "source": [
        "# Mapping from characters to integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "# encoder: it will take the string and output a list of integer\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "# decoder: it will take the list of integer and output the string\n",
        "decode = lambda l: ''.join([itos[u] for u in l])\n",
        "\n",
        "print(encode('I am Anirban Chakraborty'))\n",
        "print(decode(encode('I am Anirban Chakraborty')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vyNl4E_T0Li3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([934571982]) torch.int64\n",
            "tensor([17, 15, 17,  ..., 90,  1, 72])\n"
          ]
        }
      ],
      "source": [
        "# Now let us encode the entire text dataset and store it in a Tensor\n",
        "# for this operation we will use torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text),dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:2000]) #first 2000 characters after encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.000123, which corresponds to a distance of 705 Mly, or 216 Mpc.\n",
            "000webhost is a free web hosting s\n",
            "tensor([17, 15, 17, 17, 17, 18, 19, 20, 13,  1, 88, 73, 74, 68, 73,  1, 68, 80,\n",
            "        83, 83, 70, 84, 81, 80, 79, 69, 84,  1, 85, 80,  1, 66,  1, 69, 74, 84,\n",
            "        85, 66, 79, 68, 70,  1, 80, 71,  1, 24, 17, 22,  1, 46, 77, 90, 13,  1,\n",
            "        80, 83,  1, 19, 18, 23,  1, 46, 81, 68, 15,  0, 17, 17, 17, 88, 70, 67,\n",
            "        73, 80, 84, 85,  1, 74, 84,  1, 66,  1, 71, 83, 70, 70,  1, 88, 70, 67,\n",
            "         1, 73, 80, 84, 85, 74, 79, 72,  1, 84])\n"
          ]
        }
      ],
      "source": [
        "print(text[:100])\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dQca4VN61oSL"
      },
      "outputs": [],
      "source": [
        "# let us now split the data up into train and validation sets\n",
        "n = int(0.9*len(data)) # training on the first 90% , rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6vgm2zGv2AEQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([17, 15, 17, 17, 17, 18, 19, 20, 13])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When input is tensor([17]), the target is 15\n",
            "When input is tensor([17, 15]), the target is 17\n",
            "When input is tensor([17, 15, 17]), the target is 17\n",
            "When input is tensor([17, 15, 17, 17]), the target is 17\n",
            "When input is tensor([17, 15, 17, 17, 17]), the target is 18\n",
            "When input is tensor([17, 15, 17, 17, 17, 18]), the target is 19\n",
            "When input is tensor([17, 15, 17, 17, 17, 18, 19]), the target is 20\n",
            "When input is tensor([17, 15, 17, 17, 17, 18, 19, 20]), the target is 13\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"When input is {context}, the target is {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs: \n",
            "torch.Size([4, 8])\n",
            "tensor([[69, 90,  1, 83, 80, 86, 72, 73],\n",
            "        [ 1, 85, 73, 70,  1, 35, 83, 74],\n",
            "        [ 1, 51, 70, 85, 86, 83, 79,  1],\n",
            "        [83, 70, 66, 69, 74, 79, 72,  1]])\n",
            "targets: \n",
            "torch.Size([4, 8])\n",
            "tensor([[90,  1, 83, 80, 86, 72, 73, 77],\n",
            "        [85, 73, 70,  1, 35, 83, 74, 85],\n",
            "        [51, 70, 85, 86, 83, 79,  1, 80],\n",
            "        [70, 66, 69, 74, 79, 72,  1, 87]])\n",
            "\n",
            "When input is [69], the target is 90\n",
            "When input is [69, 90], the target is 1\n",
            "When input is [69, 90, 1], the target is 83\n",
            "When input is [69, 90, 1, 83], the target is 80\n",
            "When input is [69, 90, 1, 83, 80], the target is 86\n",
            "When input is [69, 90, 1, 83, 80, 86], the target is 72\n",
            "When input is [69, 90, 1, 83, 80, 86, 72], the target is 73\n",
            "When input is [69, 90, 1, 83, 80, 86, 72, 73], the target is 77\n",
            "When input is [1], the target is 85\n",
            "When input is [1, 85], the target is 73\n",
            "When input is [1, 85, 73], the target is 70\n",
            "When input is [1, 85, 73, 70], the target is 1\n",
            "When input is [1, 85, 73, 70, 1], the target is 35\n",
            "When input is [1, 85, 73, 70, 1, 35], the target is 83\n",
            "When input is [1, 85, 73, 70, 1, 35, 83], the target is 74\n",
            "When input is [1, 85, 73, 70, 1, 35, 83, 74], the target is 85\n",
            "When input is [1], the target is 51\n",
            "When input is [1, 51], the target is 70\n",
            "When input is [1, 51, 70], the target is 85\n",
            "When input is [1, 51, 70, 85], the target is 86\n",
            "When input is [1, 51, 70, 85, 86], the target is 83\n",
            "When input is [1, 51, 70, 85, 86, 83], the target is 79\n",
            "When input is [1, 51, 70, 85, 86, 83, 79], the target is 1\n",
            "When input is [1, 51, 70, 85, 86, 83, 79, 1], the target is 80\n",
            "When input is [83], the target is 70\n",
            "When input is [83, 70], the target is 66\n",
            "When input is [83, 70, 66], the target is 69\n",
            "When input is [83, 70, 66, 69], the target is 74\n",
            "When input is [83, 70, 66, 69, 74], the target is 79\n",
            "When input is [83, 70, 66, 69, 74, 79], the target is 72\n",
            "When input is [83, 70, 66, 69, 74, 79, 72], the target is 1\n",
            "When input is [83, 70, 66, 69, 74, 79, 72, 1], the target is 87\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1000)\n",
        "batch_size = 4 # how many sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for prediction?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and target y\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs: ')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets: ')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print()\n",
        "\n",
        "for b in range(batch_size): #batch dimn\n",
        "    for t in range(block_size): #time dimn\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"When input is {context.tolist()}, the target is {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simplest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 96])\n",
            "tensor(5.0479, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "ky1x[Xfq2H}#_+@#x4PFcgY>%>$@w{Ih4>ZBpAvn`5em:glZ:O^bfU'BIVXUT$ X.>x+h`oB@mmaoK;,cPl|q_UM(xdbW[N}v^[\"\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1000)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        #each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "    \n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        #idx and targets are both (B,T) tensor of integer\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C = logits.shape\n",
        "\n",
        "            logits = logits.view(B*T,C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "        \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        #idx is (B,T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get prediction\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes B,C\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # B,C\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # B,1\n",
        "            # append the sampled index to the running sequence\n",
        "            idx = torch.cat((idx,idx_next),dim=1) # B,T+1\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "logits, loss = model(xb,yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(model.generate(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens = 100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.505908250808716\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): \n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Govy ans wsh-t ss Eilm:Pan but jond inrn herq|Ed M-Bar igqsLy as subormused tentiche.\n",
            "Thendqus.\n",
            "Son icha d tendis om semerocopondust id ahiond abld Cuo DItaowithes.\n",
            "Hon 1997-Wadetasess thorofof a Upe creras Ade Je s rtenthen flo%piisprorewayepe t mencen is tha che thesheror by Resce, GCherabuedecl, 1, chanow Je bowh Theareanal, ttal co inguded eredamilenuly finas preserialsian 195Thiseckor, J aneem thile Upraucomech is asphr.\n",
            "Pshemais bean ics Bour, itt le Capr acothes alm fireraras owinden ond \n"
          ]
        }
      ],
      "source": [
        "print(decode(model.generate(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens = 500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Attention Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example\n",
        "torch.manual_seed(0)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# bag of words\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] #(t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0) #xbow[b,t] = mean(i<=t) x[b,i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-1.1258, -1.1524],\n",
              "         [-0.2506, -0.4339],\n",
              "         [ 0.8487,  0.6920],\n",
              "         [-0.3160, -2.1152],\n",
              "         [ 0.3223, -1.2633],\n",
              "         [ 0.3500,  0.3081],\n",
              "         [ 0.1198,  1.2377],\n",
              "         [ 1.1168, -0.2473]],\n",
              "\n",
              "        [[-1.3527, -1.6959],\n",
              "         [ 0.5667,  0.7935],\n",
              "         [ 0.5988, -1.5551],\n",
              "         [-0.3414,  1.8530],\n",
              "         [ 0.7502, -0.5855],\n",
              "         [-0.1734,  0.1835],\n",
              "         [ 1.3894,  1.5863],\n",
              "         [ 0.9463, -0.8437]],\n",
              "\n",
              "        [[-0.6136,  0.0316],\n",
              "         [-0.4927,  0.2484],\n",
              "         [ 0.4397,  0.1124],\n",
              "         [ 0.6408,  0.4412],\n",
              "         [-0.1023,  0.7924],\n",
              "         [-0.2897,  0.0525],\n",
              "         [ 0.5229,  2.3022],\n",
              "         [-1.4689, -1.5867]],\n",
              "\n",
              "        [[-0.6731,  0.8728],\n",
              "         [ 1.0554,  0.1778],\n",
              "         [-0.2303, -0.3918],\n",
              "         [ 0.5433, -0.3952],\n",
              "         [-0.4462,  0.7440],\n",
              "         [ 1.5210,  3.4105],\n",
              "         [-1.5312, -1.2341],\n",
              "         [ 1.8197, -0.5515]]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-1.1258, -1.1524],\n",
              "         [-0.6882, -0.7931],\n",
              "         [-0.1759, -0.2981],\n",
              "         [-0.2109, -0.7524],\n",
              "         [-0.1043, -0.8546],\n",
              "         [-0.0286, -0.6608],\n",
              "         [-0.0074, -0.3896],\n",
              "         [ 0.1331, -0.3718]],\n",
              "\n",
              "        [[-1.3527, -1.6959],\n",
              "         [-0.3930, -0.4512],\n",
              "         [-0.0624, -0.8192],\n",
              "         [-0.1321, -0.1511],\n",
              "         [ 0.0443, -0.2380],\n",
              "         [ 0.0080, -0.1678],\n",
              "         [ 0.2054,  0.0828],\n",
              "         [ 0.2980, -0.0330]],\n",
              "\n",
              "        [[-0.6136,  0.0316],\n",
              "         [-0.5531,  0.1400],\n",
              "         [-0.2222,  0.1308],\n",
              "         [-0.0064,  0.2084],\n",
              "         [-0.0256,  0.3252],\n",
              "         [-0.0696,  0.2798],\n",
              "         [ 0.0150,  0.5687],\n",
              "         [-0.1705,  0.2993]],\n",
              "\n",
              "        [[-0.6731,  0.8728],\n",
              "         [ 0.1911,  0.5253],\n",
              "         [ 0.0506,  0.2196],\n",
              "         [ 0.1738,  0.0659],\n",
              "         [ 0.0498,  0.2016],\n",
              "         [ 0.2950,  0.7364],\n",
              "         [ 0.0341,  0.4549],\n",
              "         [ 0.2573,  0.3291]]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xbow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a=\n",
            "tensor([[0.3333, 0.3333, 0.3333],\n",
            "        [0.0000, 0.5000, 0.5000],\n",
            "        [0.0000, 0.0000, 1.0000]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[4.6667, 5.3333],\n",
            "        [6.0000, 4.5000],\n",
            "        [6.0000, 5.0000]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.triu(torch.ones(3, 3))\n",
        "a = a / torch.sum(a,1,keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei / wei.sum(1,keepdim=True)\n",
        "xbow2 = wei @ x # (B,T,T) @ (B,T,C) --->  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.allclose(xbow2,xbow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[-1.1258, -1.1524],\n",
              "         [-0.6882, -0.7931],\n",
              "         [-0.1759, -0.2981],\n",
              "         [-0.2109, -0.7524],\n",
              "         [-0.1043, -0.8546],\n",
              "         [-0.0286, -0.6608],\n",
              "         [-0.0074, -0.3896],\n",
              "         [ 0.1331, -0.3718]]),\n",
              " tensor([[-1.1258, -1.1524],\n",
              "         [-0.6882, -0.7931],\n",
              "         [-0.1759, -0.2981],\n",
              "         [-0.2109, -0.7524],\n",
              "         [-0.1043, -0.8546],\n",
              "         [-0.0286, -0.6608],\n",
              "         [-0.0074, -0.3896],\n",
              "         [ 0.1331, -0.3718]]))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xbow[0], xbow2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: softmax\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0,float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow3, xbow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4 : self - attention\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "\n",
        "# query vector : What am I looking for ?\n",
        "# key vector : What do I contain ?\n",
        "# wei = queryvector * keyvector\n",
        "# Single head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C,head_size,bias=False)\n",
        "query = nn.Linear(C,head_size,bias=False)\n",
        "value = nn.Linear(C,head_size,bias=False)\n",
        "k = key(x) #(B,T,head_size)\n",
        "q = query(x) #(B,T,head_size)\n",
        "\n",
        "wei = q @ k.transpose(-2, -1) # (B,T,16) @ (B,16,T) -> (B,T,T)\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0,float('-inf'))\n",
        "wei = F.softmax(wei,dim=-1)\n",
        "\n",
        "v =  value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3, 2, 4, 4, 5, 0, 7, 5, 6, 4, 1]\n"
          ]
        }
      ],
      "source": [
        "text = \"hello world\"\n",
        "chars = sorted(list(set(text)))  # unique chars\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# Encoding\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "encoded = encode(\"hello world\")\n",
        "print(encoded)  # e.g., [2, 1, 3, 3, 4, 0, 5, 4, 6, 3, 7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([11, 11])\n",
            "tensor([ 1.1859,  0.2931, -1.4211,  1.6499, -1.7478,  0.5847,  0.0357, -1.5534,\n",
            "         0.6134, -0.0026, -0.1501], grad_fn=<SelectBackward0>)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "vocab_size = len(chars)\n",
        "n_embd = 11  # vector dimension for embeddings\n",
        "\n",
        "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "# Convert to tensor\n",
        "encoded_tensor = torch.tensor(encoded)  # (T,)\n",
        "tok_emb = token_embedding_table(encoded_tensor)  # (T, n_embd)\n",
        "print(tok_emb.shape)  # (11, 4)\n",
        "print(tok_emb[0])\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
